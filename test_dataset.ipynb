{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugo/Desktop/Projects/conditionned_theatre_play_generation/.env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import re \n",
    "from model_V2 import Config\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openwebtext\", num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 7933632\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take only 10% of the dataset\n",
    "#AttributeError: 'DatasetDict' object has no attribute 'train_test_split\n",
    "dataset = dataset['train'].train_test_split(test_size=0.99)\n",
    "dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 80137\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'j', 'r', '(', '1', 'u', 'y', '3', '0', '\"', '6', 't', '[', 'e', 'w', '?', 'k', ']', '2', 'b', 'o', '}', 'i', 'm', ',', 'l', ')', '8', '!', '9', 'd', 'g', ';', 'f', '\\n', 'p', ' ', 'z', 'a', \"'\", ':', 'c', 's', 'q', '4', 'h', 'v', 'n', '7', '5', '{', '-', 'x', '.'}\n",
      "53\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, '[': 23, ']': 24, 'a': 25, 'b': 26, 'c': 27, 'd': 28, 'e': 29, 'f': 30, 'g': 31, 'h': 32, 'i': 33, 'j': 34, 'k': 35, 'l': 36, 'm': 37, 'n': 38, 'o': 39, 'p': 40, 'q': 41, 'r': 42, 's': 43, 't': 44, 'u': 45, 'v': 46, 'w': 47, 'x': 48, 'y': 49, 'z': 50, '{': 51, '}': 52}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: ':', 21: ';', 22: '?', 23: '[', 24: ']', 25: 'a', 26: 'b', 27: 'c', 28: 'd', 29: 'e', 30: 'f', 31: 'g', 32: 'h', 33: 'i', 34: 'j', 35: 'k', 36: 'l', 37: 'm', 38: 'n', 39: 'o', 40: 'p', 41: 'q', 42: 'r', 43: 's', 44: 't', 45: 'u', 46: 'v', 47: 'w', 48: 'x', 49: 'y', 50: 'z', 51: '{', 52: '}'}\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing the splits (num_proc=8): 100%|██████████| 80096/80096 [00:17<00:00, 4464.36 examples/s]\n",
      "tokenizing the splits (num_proc=8): 100%|██████████| 41/41 [00:01<00:00, 22.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test') # rename the test split to val\n",
    "\n",
    "#self.preprocessor = PreprocessData(config)\n",
    "split = 'train'\n",
    "if split == 'train':\n",
    "    processed_corpus = split_dataset['train']\n",
    "elif split == 'test':\n",
    "    processed_corpus = split_dataset['val']\n",
    "\n",
    "\n",
    "#self.processed_corpus = self. preprocessor.tokenize(self.processed_corpus['text'])\n",
    "vocab_chars = set(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j','k','l','m','n','o','p','q','r','s','t','u','v','w','x', 'y', 'z', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', '[', '\\n', ']', '{', '}', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0'])\n",
    "\n",
    "print(vocab_chars)\n",
    "vocab_size = len(sorted(vocab_chars))\n",
    "stoi = {char: i for i, char in enumerate(sorted(vocab_chars))}\n",
    "itos = {i: char for i, char in enumerate(sorted(vocab_chars))}\n",
    "print(vocab_size)\n",
    "print(stoi)\n",
    "print(itos)\n",
    "print(3)\n",
    "\n",
    "all_ids = []\n",
    "def process(example) :\n",
    "    text = example['text']\n",
    "    text.replace('…', '...').replace('”', '\"').replace('’', \"'\")\n",
    "    \n",
    "    \n",
    "    ids = [stoi[char.lower()] for char in text if char.lower() in stoi]\n",
    "    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    #all_ids.append(ids)\n",
    "    #ut = {'all_ids': all_ids}\n",
    "    \n",
    "    return out\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized = split_dataset.map(\n",
    "    process,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OWTProcessData():\n",
    "    def __init__(self, ctx_len):\n",
    "        self.ctx_len = ctx_len\n",
    "\n",
    "    def remove_small_corpus(self, dataset):\n",
    "        \"\"\"Remove all sentences that are smaller than min_len\"\"\"\n",
    "        for split in dataset:\n",
    "            dataset[split] = dataset[split].filter(lambda x: x['len'] >= self.ctx_len+1, num_proc=8)\n",
    "        \n",
    "    def create_samples(self, split):\n",
    "        samples = []\n",
    "        with tqdm(total=len(split), desc=\"Creating samples\") as pbar:\n",
    "            for sample in split:\n",
    "            \n",
    "                for i in range(sample['len'] - self.ctx_len):\n",
    "                    input_seq = sample['ids'][i:i + self.ctx_len]\n",
    "                    target_seq = sample['ids'][i + 1:i + 1 + self.ctx_len]\n",
    "                    samples.append((input_seq, target_seq))\n",
    "                pbar.update(1)\n",
    "        return samples\n",
    "\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the tokenized dataset\n",
    "class OpenWebTextDataset(Dataset):\n",
    "    def __init__(self, ctx_len, tokenized_data, split_str : str):\n",
    "\n",
    "        self.data = tokenized_data\n",
    "        self.processor = OWTProcessData(ctx_len)\n",
    "        #self.data = self.processor.remove_small_corpus(self.data)\n",
    "        self.dataset = self.processor.create_samples(self.data[split_str])\n",
    "        #Save the dataset (it has)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input, target = self.dataset[idx]\n",
    "        return input, target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating samples:  17%|█▋        | 13513/80096 [34:15<33:29, 33.14it/s]    "
     ]
    }
   ],
   "source": [
    "dataset = OpenWebTextDataset(128, tokenized, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_generation_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(dataset, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
